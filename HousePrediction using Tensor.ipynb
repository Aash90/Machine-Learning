{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":40,"outputs":[{"output_type":"stream","text":"[]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf","execution_count":41,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.contrib.learn import datasets\nboston = datasets.load_boston()","execution_count":46,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = np.array(boston.data)\ntarget = np.array(boston.target)","execution_count":47,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"## Input initialization\nY = tf.placeholder(shape=[None], name=\"y-input\", dtype='float') # only 1 output\nX = tf.placeholder(shape=[None, 13], name=\"x-input\",dtype='float') # fixing number of columns( set of features )\n\nprint(features[0])\n#Data is in not in same range. So we need to normalize the data\n\nX_norm = tf.layers.batch_normalization(X, training=True)\n\n\n#target.shape = (target.shape[0], 1) # to set value in Y\n\n## Weights and Bias\nW = tf.Variable( tf.zeros(shape=[13,1]), name=\"Weights\") # its W.tranpsoe * X,  so (1 x 13) *(13 x 1)\nb = tf.Variable( tf.zeros(shape=[1]), name=\"Bias\")  # its equal to number of outputs we need\n\n\n## Predicting \nY_pred = tf.add( tf.matmul(X_norm, W), b, name=\"output\") # use normalized X \n\n\n# Reducing Mean Sqaure Error\nloss = tf.reduce_mean(tf.square(Y - Y_pred), name='Loss')\n\n\n# Gradient descent optimization, to minimize 'loss'\ngradient_opt = tf.train.GradientDescentOptimizer(0.03).minimize(loss) # 0.03 is learning rate\n","execution_count":70,"outputs":[{"output_type":"stream","text":"[6.320e-03 1.800e+01 2.310e+00 0.000e+00 5.380e-01 6.575e+00 6.520e+01\n 4.090e+00 1.000e+00 2.960e+02 1.530e+01 3.969e+02 4.980e+00]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Execute the Computational graph using above Nodes\n\n\nwith tf.Session() as tf_sess:\n    # initail variables before using them\n    tf_sess.run(tf.global_variables_initializer())\n    ## Training and Loss\n\n    training_epochs = 10000\n\n    for epoch in range(training_epochs):\n        _, train_loss = tf_sess.run([gradient_opt, loss], feed_dict={X:features, Y:target})\n\n        if epoch % 100 == 0:\n            print('Training loss at step: ', epoch, ' is ', train_loss)\n","execution_count":71,"outputs":[{"output_type":"stream","text":"Training loss at step:  0  is  592.147\nTraining loss at step:  100  is  84.442\nTraining loss at step:  200  is  84.42262\nTraining loss at step:  300  is  84.42013\nTraining loss at step:  400  is  84.41968\nTraining loss at step:  500  is  84.41958\nTraining loss at step:  600  is  84.419556\nTraining loss at step:  700  is  84.41954\nTraining loss at step:  800  is  84.41953\nTraining loss at step:  900  is  84.41952\nTraining loss at step:  1000  is  84.419495\nTraining loss at step:  1100  is  84.41949\nTraining loss at step:  1200  is  84.41947\nTraining loss at step:  1300  is  84.41947\nTraining loss at step:  1400  is  84.41947\nTraining loss at step:  1500  is  84.41948\nTraining loss at step:  1600  is  84.41947\nTraining loss at step:  1700  is  84.41949\nTraining loss at step:  1800  is  84.41949\nTraining loss at step:  1900  is  84.4195\nTraining loss at step:  2000  is  84.41952\nTraining loss at step:  2100  is  84.41952\nTraining loss at step:  2200  is  84.419525\nTraining loss at step:  2300  is  84.41953\nTraining loss at step:  2400  is  84.419525\nTraining loss at step:  2500  is  84.419525\nTraining loss at step:  2600  is  84.419525\nTraining loss at step:  2700  is  84.419525\nTraining loss at step:  2800  is  84.419525\nTraining loss at step:  2900  is  84.41953\nTraining loss at step:  3000  is  84.41953\nTraining loss at step:  3100  is  84.41953\nTraining loss at step:  3200  is  84.419525\nTraining loss at step:  3300  is  84.41953\nTraining loss at step:  3400  is  84.41953\nTraining loss at step:  3500  is  84.41953\nTraining loss at step:  3600  is  84.41953\nTraining loss at step:  3700  is  84.41953\nTraining loss at step:  3800  is  84.419525\nTraining loss at step:  3900  is  84.41952\nTraining loss at step:  4000  is  84.41952\nTraining loss at step:  4100  is  84.419525\nTraining loss at step:  4200  is  84.419525\nTraining loss at step:  4300  is  84.419525\nTraining loss at step:  4400  is  84.419525\nTraining loss at step:  4500  is  84.419525\nTraining loss at step:  4600  is  84.41952\nTraining loss at step:  4700  is  84.419525\nTraining loss at step:  4800  is  84.419525\nTraining loss at step:  4900  is  84.419525\nTraining loss at step:  5000  is  84.419525\nTraining loss at step:  5100  is  84.419525\nTraining loss at step:  5200  is  84.419525\nTraining loss at step:  5300  is  84.419525\nTraining loss at step:  5400  is  84.41952\nTraining loss at step:  5500  is  84.419525\nTraining loss at step:  5600  is  84.419525\nTraining loss at step:  5700  is  84.41953\nTraining loss at step:  5800  is  84.419525\nTraining loss at step:  5900  is  84.41953\nTraining loss at step:  6000  is  84.41953\nTraining loss at step:  6100  is  84.41953\nTraining loss at step:  6200  is  84.419525\nTraining loss at step:  6300  is  84.419525\nTraining loss at step:  6400  is  84.419525\nTraining loss at step:  6500  is  84.419525\nTraining loss at step:  6600  is  84.41953\nTraining loss at step:  6700  is  84.41953\nTraining loss at step:  6800  is  84.419525\nTraining loss at step:  6900  is  84.41953\nTraining loss at step:  7000  is  84.419525\nTraining loss at step:  7100  is  84.419525\nTraining loss at step:  7200  is  84.41953\nTraining loss at step:  7300  is  84.41953\nTraining loss at step:  7400  is  84.419525\nTraining loss at step:  7500  is  84.419525\nTraining loss at step:  7600  is  84.41952\nTraining loss at step:  7700  is  84.41953\nTraining loss at step:  7800  is  84.41953\nTraining loss at step:  7900  is  84.419525\nTraining loss at step:  8000  is  84.41953\nTraining loss at step:  8100  is  84.41953\nTraining loss at step:  8200  is  84.419525\nTraining loss at step:  8300  is  84.419525\nTraining loss at step:  8400  is  84.41953\nTraining loss at step:  8500  is  84.41953\nTraining loss at step:  8600  is  84.419525\nTraining loss at step:  8700  is  84.41953\nTraining loss at step:  8800  is  84.419525\nTraining loss at step:  8900  is  84.41953\nTraining loss at step:  9000  is  84.41953\nTraining loss at step:  9100  is  84.419525\nTraining loss at step:  9200  is  84.41953\nTraining loss at step:  9300  is  84.419525\nTraining loss at step:  9400  is  84.419525\nTraining loss at step:  9500  is  84.41953\nTraining loss at step:  9600  is  84.41953\nTraining loss at step:  9700  is  84.41953\nTraining loss at step:  9800  is  84.41953\nTraining loss at step:  9900  is  84.41952\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":56,"outputs":[{"output_type":"execute_result","execution_count":56,"data":{"text/plain":"array([6.320e-03, 1.800e+01, 2.310e+00, 0.000e+00, 5.380e-01, 6.575e+00,\n       6.520e+01, 4.090e+00, 1.000e+00, 2.960e+02, 1.530e+01, 3.969e+02,\n       4.980e+00])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(features[0])\n#Data is in not in same range. So we need to normalize the data\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}